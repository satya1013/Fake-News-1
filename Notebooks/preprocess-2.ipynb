{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import contractions\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                tweet  label\n",
      "id                                                          \n",
      "1   currently general death different small explic...      1\n",
      "2                            small rise last southern      1\n",
      "3   politically correct woman almost pandemic excu...      0\n"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"../Dataset/csv/train2.csv\", header=0, index_col=0)\n",
    "df_1.dropna(inplace = True)\n",
    "#print(df_1.isnull().sum(axis = 0))\n",
    "\n",
    "#Expanding Contraction, Lower Case and Word Splitting\n",
    "df_1['tweet'] = df_1['tweet'].apply(lambda x: [contractions.fix(word, slang=False).lower() for word in x.split()])\n",
    "\n",
    "#Removing Punctuations\n",
    "df_1['tweet'] = df_1['tweet'].apply(lambda x: [re.sub(r'[^\\w\\s]','', word)  for word in x])\n",
    "\n",
    "#Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_1['tweet'] = df_1['tweet'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Removing Special Charecter and Numbers\n",
    "df_1['tweet'] = df_1['tweet'].apply(lambda x: [word for word in x if re.search(\"[@_!#$%^&*()<>?/|}{~:0-9]\", word) == None])\n",
    "\n",
    "#Removing Non-English Words\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "df_1['tweet'] = df_1['tweet'].apply(lambda x: [word for word in x if word in english_words])\n",
    "\n",
    "#Concating Words back to Sentence\n",
    "df_1['tweet'] = df_1['tweet'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "#Transforming Labels\n",
    "df_1['label'] = df_1['label'].apply(lambda x: 1 if (x == 'real') else 0)\n",
    "\n",
    "print(df_1.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(\"../Dataset/csv/test2.csv\", header=0, index_col=0)\n",
    "df_2.dropna(inplace = True)\n",
    "#print(df_2.isnull().sum(axis = 0))\n",
    "\n",
    "#Expanding Contraction, Lower Case and Word Splitting\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [contractions.fix(word, slang=False).lower() for word in x.split()])\n",
    "\n",
    "#Removing Punctuations\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [re.sub(r'[^\\w\\s]','', word)  for word in x])\n",
    "\n",
    "#Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Removing Special Charecter and Numbers\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [word for word in x if re.search(\"[@_!#$%^&*()<>?/|}{~:0-9]\", word) == None])\n",
    "\n",
    "#Removing Non-English Words\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [word for word in x if word in english_words])\n",
    "\n",
    "#Concating Words back to Sentence\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "#Transforming Labels\n",
    "df_2['label'] = df_2['label'].apply(lambda x: 1 if (x == 'real') else 0)\n",
    "\n",
    "#print(df_2.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfVectorizer = TfidfVectorizer(use_idf=True,stop_words='english')\n",
    "tfidfVectorizer.fit(df_1['tweet'])  \n",
    "tfidfVectorizer.fit(df_2['tweet']) \n",
    "#print(tfidfVectorizer.get_feature_names())\n",
    "\n",
    "trainTfidfVector = tfidfVectorizer.transform(df_1['tweet']) \n",
    "testTfidfVector = tfidfVectorizer.transform(df_2['tweet']) \n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_1['tweet'])\n",
    "tokenizer.fit_on_texts(df_2['tweet'])\n",
    "\n",
    "trainSequence = np.array([np.array(seq) for seq in pad_sequences(tokenizer.texts_to_sequences(df_1['tweet']))])\n",
    "testSequence = np.array([np.array(seq) for seq in pad_sequences(tokenizer.texts_to_sequences(df_2['tweet']))])\n",
    "\n",
    "pickle.dump(tfidfVectorizer, open(\"../Dataset/tfidf_vectorizer-2.pickle\", \"wb\"))\n",
    "pickle.dump(tokenizer, open(\"../Dataset/tokenizer-2.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(trainTfidfVector, open(\"../Dataset/tfidf_train-2.pickle\", \"wb\"))\n",
    "pickle.dump(trainSequence, open(\"../Dataset/sequence_train-2.pickle\", \"wb\"))\n",
    "pickle.dump(df_1['label'], open(\"../Dataset/label_train-2.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(testTfidfVector, open(\"../Dataset/tfidf_test-2.pickle\", \"wb\"))\n",
    "pickle.dump(testSequence, open(\"../Dataset/sequence_test-2.pickle\", \"wb\"))\n",
    "pickle.dump(df_2['label'], open(\"../Dataset/label_test-2.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfVectorizer = pickle.load(open(\"../Dataset/tfidf_vectorizer-2.pickle\", \"rb\"))\n",
    "#print(tfidfVectorizer.get_feature_names())\n",
    "\n",
    "tokenizer = pickle.load(open(\"../Dataset/tokenizer-2.pickle\", \"rb\"))\n",
    "\n",
    "trainTfidfVector = pickle.load(open(\"../Dataset/tfidf_train-2.pickle\", \"rb\"))\n",
    "df_train = pd.DataFrame(data = trainTfidfVector.toarray(),columns = tfidfVectorizer.get_feature_names())\n",
    "#print(df_train)\n",
    "\n",
    "testTfidfVector = pickle.load(open(\"../Dataset/tfidf_test-2.pickle\", \"rb\"))\n",
    "df_test = pd.DataFrame(data = testTfidfVector.toarray(),columns = tfidfVectorizer.get_feature_names())\n",
    "#print(df_test)\n",
    "\n",
    "trainSequence = pickle.load(open(\"../Dataset/sequence_train-2.pickle\", \"rb\"))\n",
    "#print(trainSequence)\n",
    "\n",
    "testSequence = pickle.load(open(\"../Dataset/sequence_test-2.pickle\", \"rb\"))\n",
    "#print(testSequence)\n",
    "\n",
    "trainLabels = pickle.load(open(\"../Dataset/label_train-2.pickle\", \"rb\"))\n",
    "#print(trainLabels)\n",
    "\n",
    "testLabels = pickle.load(open(\"../Dataset/label_test-2.pickle\", \"rb\"))\n",
    "#print(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
